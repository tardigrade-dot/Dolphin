{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebfcb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/Users/dev4/miniconda3/envs/dolphin311/bin/dolphin-run \\\n",
    "!dolphin-run \\\n",
    "    --model_path \"/Volumes/sw/pretrained_models/Dolphin-1.5\" \\\n",
    "        --input_path '/Volumes/sw/books/罗洛·梅文集 权力与无知：寻求暴力的根源 ([美]罗洛·梅著； 郭本宇 方红译（中国人民大学出版社2013年）) (Z-Library).pdf' \\\n",
    "            --save_dir /Volumes/sw/ocr_result/results_quanliyuwuzhi \\\n",
    "                --max_batch_size 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bad38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#把所有的json文件内容提取, 并过滤掉不需要的, 仅保存文本内容\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from dolphin.utils.markdown_utils import MarkdownConverter\n",
    "\n",
    "# from utils.utils import save_combined_pdf_results\n",
    "#/Volumes/sw/ocr_result/results_quanliyuwuzhi\n",
    "\n",
    "# ignore 'header'\n",
    "label_filter = ['para', 'title', 'half_para', 'catalogue', 'sec','sub_sec', 'sec_0', \n",
    "                'sec_1', 'sec_2', 'sec_3', 'sec_4', 'sec_5', 'fig', 'tab', 'equ', 'list', 'code']\n",
    "\n",
    "def save_combined_pdf_results_to_markdown(all_page_results, save_name, save_dir):\n",
    "    \"\"\"Save combined results for multi-page PDF with both JSON and Markdown\n",
    "\n",
    "    Args:\n",
    "        all_page_results: List of results for all pages\n",
    "        pdf_path: Path to original PDF file\n",
    "        save_dir: Directory to save results\n",
    "\n",
    "    Returns:\n",
    "        Path to saved combined JSON file\n",
    "    \"\"\"\n",
    "    # Create output filename based on PDF name\n",
    "    base_name = save_name\n",
    "\n",
    "    # Prepare combined results\n",
    "    combined_results = {\"source_file\": save_name, \"total_pages\": len(all_page_results), \"pages\": all_page_results}\n",
    "\n",
    "    # Save combined JSON results\n",
    "    json_filename = f\"{base_name}.json\"\n",
    "    json_path = os.path.join(save_dir, \"recognition_json\", json_filename)\n",
    "    os.makedirs(os.path.dirname(json_path), exist_ok=True)\n",
    "\n",
    "    with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(combined_results, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    # Generate and save combined markdown\n",
    "    try:\n",
    "        markdown_converter = MarkdownConverter()\n",
    "\n",
    "        # Combine all page results into a single list for markdown conversion\n",
    "        all_elements = []\n",
    "        for page_data in all_page_results:\n",
    "            page_elements = page_data.get(\"elements\", [])\n",
    "            if page_elements:\n",
    "                # Add page separator if not the first page\n",
    "                # if all_elements:\n",
    "                #     all_elements.append(\n",
    "                #         {\"label\": \"page_separator\", \"text\": f\"\\n\\n---\\n\\n\", \"reading_order\": len(all_elements)}\n",
    "                #     )\n",
    "                all_elements.extend(page_elements)\n",
    "\n",
    "        # Generate markdown content\n",
    "        markdown_content = markdown_converter.convert(all_elements)\n",
    "\n",
    "        # Save markdown file\n",
    "        markdown_filename = f\"{base_name}.md\"\n",
    "        markdown_path = os.path.join(save_dir, \"markdown\", markdown_filename)\n",
    "        os.makedirs(os.path.dirname(markdown_path), exist_ok=True)\n",
    "\n",
    "        with open(markdown_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(markdown_content)\n",
    "\n",
    "        # print(f\"Combined markdown saved to: {markdown_path}\")\n",
    "\n",
    "    except ImportError:\n",
    "        print(\"MarkdownConverter not available, skipping markdown generation\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating markdown: {e}\")\n",
    "\n",
    "    # print(f\"Combined JSON results saved to: {json_path}\")\n",
    "    return markdown_path\n",
    "\n",
    "def combine_json(book_name):\n",
    "    base_dir = f'/Volumes/sw/ocr_result/results_{book_name}'\n",
    "    print(f'处理json目录: {base_dir}')\n",
    "    json_dir = f\"{base_dir}/recognition_json/\"\n",
    "    output_file = os.path.join(os.path.dirname(json_dir.rstrip('/')), \"merged_paragraphs.txt\")\n",
    "\n",
    "    all_texts = []\n",
    "    def n_of_filename(_filename):\n",
    "        _filename = Path(_filename).stem \n",
    "        number = _filename.split(\"_\")[-1]\n",
    "        return number\n",
    "\n",
    "    def is_punctuation_end(sentence):\n",
    "        punctuation_marks = {'.', ',', '?', '!', ';', '。', '，', '？', '！', '；'}\n",
    "        sentence = sentence.strip()\n",
    "        if not sentence:  # 处理空字符串\n",
    "            return False\n",
    "        \n",
    "        return sentence[-1] in punctuation_marks\n",
    "\n",
    "    files = os.listdir(json_dir)\n",
    "    sorted_files = sorted(\n",
    "        # [f for f in files if f.startswith(f'{book_name}_page_') and f.endswith('.json')],\n",
    "        [f for f in files if re.search(r'_page_\\d+\\.json$', f)],\n",
    "        key=lambda f: int(n_of_filename(f))\n",
    "    )\n",
    "\n",
    "    all_elements = []\n",
    "    for filename in sorted_files:\n",
    "        json_path = os.path.join(json_dir, filename)\n",
    "\n",
    "        bbox_list = []\n",
    "        with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            try:\n",
    "                data = json.load(f)\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"跳过无效 JSON 文件: {filename}\")\n",
    "                continue\n",
    "        match = re.search(r'_page_(\\d+)\\.json$', filename)\n",
    "        if match:\n",
    "            page_number = int(match[1])\n",
    "\n",
    "            data = [_d for _d in data if _d['label'] in label_filter]\n",
    "            page_results = {\n",
    "                \"page_number\": page_number + 1,\n",
    "                \"elements\": data\n",
    "            }\n",
    "        else:\n",
    "            print(f'not match file {filename}')\n",
    "        all_elements.append(page_results)\n",
    "        paras = sorted(\n",
    "            [\n",
    "                item\n",
    "                for item in data\n",
    "                if isinstance(item, dict) and item.get('label') in label_filter\n",
    "            ],\n",
    "            key=lambda x: x['reading_order']\n",
    "        )\n",
    "        result_text = ''\n",
    "        for p in paras:\n",
    "            _text = p['text'].replace('\\n', '')\n",
    "            if p.get('label') == 'sub_sec':\n",
    "                _text = '章节: ' + _text + ';'\n",
    "            if is_punctuation_end(result_text):\n",
    "                result_text = result_text + '\\n' + _text\n",
    "            else:\n",
    "                result_text = result_text + _text\n",
    "        if result_text:\n",
    "            all_texts.append(result_text)\n",
    "            \n",
    "    md_path = save_combined_pdf_results_to_markdown(all_elements, book_name, base_dir)\n",
    "    print(f'saved markdown file [{md_path}]')\n",
    "\n",
    "    merged_text = ''\n",
    "    for _l in all_texts:\n",
    "        if is_punctuation_end(merged_text):\n",
    "            merged_text = merged_text + f'\\n\\n{_l}'\n",
    "        else:\n",
    "            merged_text = merged_text + _l\n",
    "    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as out_f:\n",
    "        out_f.write(merged_text)\n",
    "    print(f\"保存完成，总共 {len(all_texts)} 个 JSON 文件内容合并到 {output_file}\")\n",
    "\n",
    "combine_json(\"quanliyuwuzhi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1766e20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#提取出pdf的某一页,以做测试或者其他验证\n",
    "from pdf2image import convert_from_path\n",
    "\n",
    "def extract_pdf_page_to_image(pdf_path, page_number, output_path):\n",
    "    try:\n",
    "        # Convert specific page to image (page_number is 1-based in pdf2image)\n",
    "        pages = convert_from_path(pdf_path, first_page=page_number, last_page=page_number)\n",
    "        \n",
    "        if not pages:\n",
    "            raise ValueError(f\"No page found for page number {page_number}\")\n",
    "        \n",
    "        # Save the page as an image\n",
    "        pages[0].save(output_path, 'PNG')  # Save as PNG (can change to JPEG, etc.)\n",
    "        print(f\"Page {page_number} saved as {output_path}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "\n",
    "# Example usage\n",
    "pdf_path = \"/Volumes/sw/MyDrive/data_src/独立宣言 一种全球史 (（美）大卫·阿米蒂奇) (Z-Library).pdf\"\n",
    "page_number = 20  # Replace with desired page number (1-based)\n",
    "output_path = f\"/Volumes/sw/MyDrive/data_src/page_{page_number}.png\"  # Output image file path\n",
    "\n",
    "extract_pdf_page_to_image(pdf_path, page_number, output_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dolphin311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
